---
title: "*Ridge and Lasso*"
author: "F. Chiaromonte (33%), J. Di Iorio (33%), L. Insolia (33%), L. Testa (1%)"
date: "March 14th 2023"
output:
  pdf_document:
    toc: true
  html_document: default
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) # just to clean the pdf files, not recommended in practice!
```

\section{Introduction}

\subsection{Libraries}

We are going to use a few libraries:

-   glmnet: Lasso and Elastic-Net Regularized Generalized Linear Models

-   tidyverse:

-   caret: Classification and Regression Training

-   ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics

-   corrplot: Correlation matrix plotting

```{r}
library(glmnet)     # ridge and lasso for GLMs
library(tidyverse)  # data manipulation and visualization
library(caret)      # statistical learning techniques
library(ggplot2)    # plots
library(corrplot)   # correlation matrix plotting
```

\subsection{Data}

We will use the \textbf{Body Fat dataset} (which is available in the [Datasets folder](https://github.com/EMbeDS-education/StatsAndComputing20212022/wiki/Datasets) of our course).

The data concerns a sample of 252 men, and contains 15 variables:

-   Density of the body, determined from underwater weighing
-   Percentage of body fat, calculated as a function of the Density according to Siri's equation: (495/Density) -- 450.
-   Indicator for Age group (binary; 0: up to 45 years, 1: over 45)
-   Weight (lbs)
-   Height (inches)
-   Neck circumference (cm)
-   Chest circumference (cm)
-   Abdomen circumference (cm)
-   Hip circumference (cm)
-   Thigh circumference (cm)
-   Knee circumference (cm)
-   Ankle circumference (cm)
-   Biceps circumference (cm)
-   Forearm circumference (cm)
-   Wrist circumference (cm)

We want to understand whether we can reliably describe and predict body fat percentage on the basis of these variables, using regression methods. For age, we only have a binary indicator separating men below and above 45 years. The body measurements, on the other hand, are all continuous variables. Please see the [data description file](https://github.com/EMbeDS-education/StatsAndComputing20212022/raw/main/datasets/DesciptionBodyFatdataset.doc) for more details.

```{r}
df <- read.table('BODY_FAT.TXT', header=TRUE)
names(df)
```

We want to predict "SiriBF." using the other features, aside from "Density". So we drop the "Density" column.

```{r}
df <- df[,-1]
```

\section{Penalized regression}

We will perform ridge/lasso penalization through the \textbf{glmnet} package. Let us identify predictors and response variable

```{r}
# getting the predictors
x_var <- data.matrix(df[,-1])     # NOTE: glmnet requires a matrix structure
# getting the response variable
y_var <- df[,"SiriBF."]
```

Let's have a look a the glmnet function:

```{r}
help(glmnet)
```

Note that:

-   input matrix
-   response variable
-   $\alpha$ is the elastic-net mixing parameter with range [0, 1]. Namely, $\alpha = 1$ is the lasso (default) and $\alpha = 0$ is the ridge.
-   standardize is a logical flag for $x$ variable standardization, prior to fitting the model sequence. The coefficients are always returned on the original scale. Default is standardize=TRUE.

\subsection{Ridge}

To perform ridge regression, we run glmnet with $\alpha = 0$. The $\lambda$'s sequence is internally computed by the package itself -- although a user-defined sequence can be provided as a $lambda$ argument.

```{r}
ridge <- glmnet(x_var, y_var, alpha=0)
summary(ridge)
```

The summary is quite different than the one for linear regression, since ridge regression requires the tuning of $\lambda$. The code above fits a ridge regression for each $\lambda$ value, and we have access to each of these model estimates.

We can plot the regularization path as follows:

```{r}
dim(ridge$beta)
plot(ridge, xvar="lambda")
```

We can automate the task of finding the optimal lambda value using the \textbf{cv.glmnet} function. This performs a k-fold cross-validation for glmnet, produces a plot, and returns \`\`optimal'' $\lambda$ values.

```{r}
cv_ridge <- cv.glmnet(x_var, y_var, alpha = 0)
cv_ridge
```

Two particular values of $\lambda$ are highlighted: the minimum (min) and the largest value of lambda such that error is within 1 standard error of the minimum (1se).

```{r}
cv_ridge$lambda.min
cv_ridge$lambda.1se
```

We can visualize them in this way:

```{r}
plot(cv_ridge)
```

Let us see again how the regression coefficients change by modifying $\lambda$, highlighting the min and 1se values:

```{r}
lbs_fun <- function(fit, offset_x=1, ...) {
  L <- length(fit$lambda)
  x <- log(fit$lambda[L])+ offset_x
  y <- fit$beta[, L]
  labs <- names(y)
  text(x, y, labels=labs, cex=0.75, ...)
}

plot(ridge, xvar = "lambda", label=T)
lbs_fun(ridge)
abline(v=log(cv_ridge$lambda.min), col = "red", lty=2)
abline(v=log(cv_ridge$lambda.1se), col="blue", lty=2)
legend(x = "bottomright",
       legend = c("lambda min", "lambda 1se"),
       lty = c(2, 2),
       col = c("red", "blue"))
```

WristC has a very strong negative effect on the response. Why is that? We can find a partial answer by looking at the correlation matrix!

```{r}
corrplot(cor(df))
```

Let's re-fit the model and see the estimates associated to the minimum $\lambda$.

```{r}
min_ridge <- glmnet(x_var, y_var, alpha=0, lambda= cv_ridge$lambda.min)
coef(min_ridge)
```

We can use this model to make predictions on the training set.

```{r}
# Make predictions on the training data
predictions <- min_ridge %>% predict(x_var) %>% as.vector()
# Model performance metrics
data.frame(
  RMSE = RMSE(predictions, y_var),
  Rsquare = R2(predictions, y_var)
)
```

Be careful though! We are making predictions and assessing the goodness of fit based on training data. Is it the best choice? Do you have any other suggestions?

\subsection{Lasso}

Let us now perform Lasso regression using the \textbf{glmnet} package. We follow the same approach as in Ridge regression, but set $\alpha = 1$.

```{r}
lasso <- glmnet(x_var, y_var, alpha=1)
summary(lasso)
```

Let's have a look at the selection path:

```{r}
plot(lasso, xvar="lambda")
```

Once again, we need to tune the sparsity parameter $\lambda$. We use k-fold cross-validation through the \textbf{cv.glmnet} function.

```{r}
cv_lasso <- cv.glmnet(x_var, y_var, alpha = 1)
cv_lasso
plot(cv_lasso)
```

Also here, it outputs the min and the 1se $\lambda$. As expected, the number of non-zero coefficients (which is printed on top of the previous plot) is lower than the one for Ridge regression.

Let us see again how the regression coefficients change by modifying $\lambda$:

```{r}
lbs_fun <- function(fit, offset_x=1, ...) {
  L <- length(fit$lambda)
  x <- log(fit$lambda[L])+ offset_x
  y <- fit$beta[, L]
  labs <- names(y)
  text(x, y, labels=labs, cex=0.75, ...)
}

plot(lasso, xvar = "lambda", label=T)
lbs_fun(lasso)
abline(v=log(cv_lasso$lambda.min), col = "red", lty=2)
abline(v=log(cv_lasso$lambda.1se), col="blue", lty=2)
legend(x = "bottomright",
       legend = c("lambda min", "lambda 1se"),
       lty = c(2, 2),
       col = c("red", "blue"))
```

Let us rebuild the model and compare the estimated coefficients for min and 1se $\lambda$.

```{r}
min_lasso <- glmnet(x_var, y_var, alpha=1, lambda= cv_lasso$lambda.min)
se_lasso <- glmnet(x_var, y_var, alpha=1, lambda= cv_lasso$lambda.1se)

lasso_mat <- cbind(coef(min_lasso), coef(se_lasso))
colnames(lasso_mat) <- c("min", "1se")
lasso_mat

```

We can use this model to make predictions on the training set.

```{r}
# Make predictions on the training data
predictions <- se_lasso %>% predict(x_var) %>% as.vector()
# Model performance metrics
data.frame(
  RMSE = RMSE(predictions, y_var),
  Rsquare = R2(predictions, y_var)
)
```

\subsection{Unbiasing the LASSO}

You have learnt that LASSO estimates are biased downwards, i.e. the coefficients are shrunk towards 0. An approach to debias them is the following:

-   run a LASSO on the original data set;
-   run a OLS on the set of variables selected by the LASSO in the previous step.

Perhaps, compare the following two regression methods:

```{r}
predictions_bias <- min_lasso %>% predict(x_var) %>% as.vector()

coefs = which(coef(min_lasso)!=0) - 1
coefs = coefs[2:length(coefs)]
prediction_2step <- lm(y_var ~ ., data = as.data.frame(x_var[, coefs])) %>% predict(as.data.frame(x_var[, coefs])) %>% as.vector()

# Model performance metrics
data.frame(
  RMSE_bias = RMSE(predictions_bias, y_var),
  Rsquare_bias = R2(predictions_bias, y_var),
  RMSE_2step = RMSE(prediction_2step, y_var),
  Rsquare_2step = R2(prediction_2step, y_var)
)
```

\subsection{A more realistic setting}

Let's try to overfit and look at the MSE increase again!

```{r}
# take all pairwise interactions
dim(x_var)
newx <- as.data.frame(x_var)
newx <- model.matrix(~ .^2, data=newx)
dim(newx)
colnames(newx)

# take their squared terms too!
newx <- as.matrix(newx)
newx <- cbind(newx, newx[, 2:ncol(newx)]^2)
dim(newx)

cv_lasso <- cv.glmnet(newx, y_var, alpha = 1)
cv_lasso
plot(cv_lasso)
coef(cv_lasso)
```

However, lasso assumptions are unlikely to hold due to strong correlations between predictors.

```{r}
# original data
corrplot(cor(x_var))

# transformed data
newx = as.data.frame(newx[, 2:ncol(newx)])
corrplot(cor(newx), tl.pos='n')
```

\section{Digging in the irrepresentability condition}

As you have seen before, the variance-covariance matrix can tell us whether LASSO will work (at least asymptotically) on the data at our disposal. Let's have a look at an example (available [here](https://www.r-bloggers.com/2017/06/when-the-lasso-fails/)).

```{r}
set.seed(12345) # Seed for replication

library(mvtnorm) # Sampling from a multivariate Normal
library(clusterGeneration) # Random matrix generation

p = 10 # = Number of Candidate Variables
k = 5 # = Number of Relevant Variables
n = 500 # = Number of observations

betas = (-1)^(1:k) # = Values for beta

sigma1 = genPositiveDefMat(p,"unifcorrmat")$Sigma # sigma1 violates irc

sigma2 = sigma1 # sigma2 satisfies irc
sigma2[(k+1):p,1:k]=0 # removing correlation among active and nonactive variables
sigma2[1:k,(k+1):p]=0

# Verify irrepresentable condition
irc1 = sort(abs(sigma1[(k+1):p,1:k] %*% solve(sigma1[1:k,1:k]) %*% sign(betas)))
irc2 = sort(abs(sigma2[(k+1):p,1:k] %*% solve(sigma2[1:k,1:k]) %*% sign(betas)))
c(max(irc1),max(irc2))

# = Have a look at the correlation matrices
par(mfrow=c(1,2))
corrplot(cov2cor(sigma1))
corrplot(cov2cor(sigma2))
```

The first variance-covariance matrix does not satisfy IRC, while the second does. Let's check it "visually", by exploring the regularization paths!

```{r}
X1 = rmvnorm(n,sigma = sigma1) # Variables violating IRC
X2 = rmvnorm(n,sigma = sigma2) # Variables satisfying IRC
e = rnorm(n) # Error from Standard Normal
y1 = X1[,1:k]%*%betas+e # Generate y for design 1
y2 = X2[,1:k]%*%betas+e # Generate y for design 2
lasso1 = glmnet(X1,y1,nlambda = 100) # Estimation for design 1
lasso2 = glmnet(X2,y2,nlambda = 100) # Estimation for design 2

# Regularization paths
par(mfrow=c(1,2))
l1=log(lasso1$lambda)
matplot(as.matrix(l1),t(coef(lasso1)[-1,]),type="l",lty=1,col=c(rep(1,9),2),ylab="coef",xlab="log(lambda)",main="Violates IRC")
l2=log(lasso2$lambda)
matplot(as.matrix(l2),t(coef(lasso2)[-1,]),type="l",lty=1,col=c(rep(1,9),2),ylab="coef",xlab="log(lambda)",main="Satisfies IRC")
```

\section{Nonconvex penalization methods}

If you want to try nonconvex penalization methods, have a look at the \textbf{ncvreg} package! It has a very similar syntax than glmnet (check the paper [here](https://projecteuclid.org/journals/annals-of-applied-statistics/volume-5/issue-1/Coordinate-descent-algorithms-for-nonconvex-penalized-regression-with-applications-to/10.1214/10-AOAS388.full)).

```{r}
library(ncvreg)

dim(newx)
cv_scad <- cv.ncvreg(as.matrix(newx), y_var)
plot(cv_scad)
```
