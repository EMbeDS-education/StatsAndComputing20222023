knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) # just to clean the pdf files, not recommended in practice!
library(caret)  # statistical learning techniques
library(leaps)  # BSS
df <- read.table('BODY_FAT.TXT', header=TRUE)
names(df)
df <- df[,-1]
regfit.full = regsubsets(SiriBF. ~ ., data = df,  nvmax = 13, method="exhaustive")
summary(regfit.full)
names(summary(regfit.full))
summary(regfit.full)$rsq
#plot rss
plot(summary(regfit.full)$rsq, type="b")
reg.summary <- summary(regfit.full)
par(mfrow=c(2,2))
plot(reg.summary$rss ,xlab="Number of Variables ",ylab="RSS",type="l")
plot(reg.summary$adjr2 ,xlab="Number of Variables ", ylab="Adjusted RSq",type="l")
max_adjr2 <- which.max(reg.summary$adjr2)
abline(v=max_adjr2, col="red", lty=2)
points(max_adjr2,reg.summary$adjr2[max_adjr2], col="red",cex=2,pch=20)
plot(reg.summary$cp ,xlab="Number of Variables ",ylab="Cp", type='l')
min_cp <- which.min(reg.summary$cp )
points(min_cp, reg.summary$cp[min_cp],col="red",cex=2,pch=20)
abline(v=min_cp, col="red", lty=2)
plot(reg.summary$bic ,xlab="Number of Variables ",ylab="BIC",type='l')
min_bic <- which.min(reg.summary$bic)
points(min_bic,reg.summary$bic[min_bic],col="red",cex=2,pch=20)
abline(v=min_bic, col="red", lty=2)
plot(regfit.full,scale="bic") #for "bic"
plot(regfit.full,scale="adjr2") #for "adjr2"
dim(df)
set.seed(1)
train=sample(1:nrow(df), round(nrow(df)*0.8), rep=F)
test = which(!(1:nrow(df) %in% train))
length(test)
length(train)
regfit.best <- regsubsets(SiriBF. ~ ., data = df[train,],  nvmax = 13, method="exhaustive")
test.mat = model.matrix(SiriBF. ~ ., data = df[test,])
dim(test.mat)
p = (ncol(df)-1) # number of predictors
mse <- rep(NA, p)
for(i in 1:p){
coefi=coef(regfit.best,id=i)
pred=test.mat[,names(coefi)]%*%coefi
mse[i]=mean((df$SiriBF.[test]-pred)^2)
}
plot(mse, type='b')
abline(v=which.min(mse), col="red", lty=2)
legend(x = "bottomright",
legend = "min test-MSPE",
lty = 2,
col = "red")
regfit.best
coef(regfit.best,id=i)
p = (ncol(df)-1) # number of predictors
mse <- rep(NA, p) # out-of-sample MSE
for(i in 1:p){
coefi=coef(regfit.best,id=i)
pred=test.mat[,names(coefi)]%*%coefi
mse[i]=mean((df$SiriBF.[test]-pred)^2)
}
plot(mse, type='b')
abline(v=which.min(mse), col="red", lty=2)
legend(x = "bottomright",
legend = "min test-MSPE",
lty = 2,
col = "red")
coef(regfit.best, which.min(mse))
regfit.best.full <- regsubsets(SiriBF. ~ ., data=df ,nvmax=13)
coef(regfit.best.full, which.min(mse))
rescompare <- rbind(coef(regfit.best, which.min(mse)),
coef(regfit.best.full, which.min(mse)))
rownames(rescompare) <- c("train", "full")
rescompare
k = 10
set.seed(123)
# folds
folds <- createFolds(1:nrow(df), k = 10, list = TRUE, returnTrain = T)
fold <- matrix(NA, nrow(df), k)
for (i in 1:k) {
fold[, i] <- (1:nrow(df) %in% folds[[i]])
}
head(fold, 10)
# initialize an empty matrix to contain test errors
cv.errors=matrix(NA, k, # num of folds
p,     # num of variables
dimnames=list(NULL, paste(1:p)))
folds
fold
head(fold, 10)
k = 10
set.seed(123)
# folds
folds <- createFolds(1:nrow(df), k = 10, list = TRUE, returnTrain = T)
fold <- matrix(NA, nrow(df), k)
for (i in 1:k) {
fold[, i] <- (1:nrow(df) %in% folds[[i]])
}
head(fold, 10)
# initialize an empty matrix to contain test errors
cv.errors=matrix(NA, k, # num of folds
p,     # num of variables
dimnames=list(NULL, paste(1:p)))
fold
k = 10
set.seed(123)
# folds
folds <- createFolds(1:nrow(df), k = 10, list = TRUE, returnTrain = T)
fold <- matrix(NA, nrow(df), k)
for (i in 1:k) {
fold[, i] <- (1:nrow(df) %in% folds[[i]])
}
head(fold, 10)
# initialize an empty matrix to contain test errors
cv.errors=matrix(NA, k, # num of folds
p,     # num of variables
dimnames=list(NULL, paste(1:p)))
cv.errors
head(fold, 10)
predict.regsubsets = function (object, newdata, id, ...){
form = as.formula(object$call[[2]])
mat = model.matrix(form,newdata)
coefi = coef(object ,id=id)
xvars = names(coefi)
mat[,xvars] %*% coefi
}
# loop for each fold
for(j in 1:k){
best.fit = regsubsets(SiriBF. ~ . , data=df[fold[,j], ], nvmax = p)
# for each best model
for (i in 1:p){
pred = predict.regsubsets(best.fit, df[!fold[,j], ], id = i)
cv.errors[j, i] = mean((df$SiriBF.[!fold[,j]] - pred)^2)
}
}
mean_mse <- colMeans(cv.errors)
plot(mean_mse, type='b')
abline(v=which.min(mean_mse), col="red", lty=2)
legend(x = "bottomright",
legend = "min CV-MSPE",
lty = 2,
col = "red")
reg.best  <- regsubsets (SiriBF. ~ ., data=df, nvmax=p)
coef(reg.best, which.min(mean_mse))
regfit.fwd = regsubsets(SiriBF. ~. , data=df,nvmax=13, method ="forward")
regfit.bwd = regsubsets(SiriBF. ~. , data=df,nvmax=13, method ="backward")
summary(regfit.fwd)
summary(regfit.bwd)
library(glmnet)
x_var <- data.matrix(df[,-1])     # NOTE: glmnet requires a matrix structure
# getting the response variable
y_var <- df[,"SiriBF."]
cv_lasso <- cv.glmnet(x_var, y_var, alpha = 1)
cv_lasso
plot(cv_lasso)
x_var <- data.matrix(df[,-1])     # NOTE: glmnet requires a matrix structure
# getting the response variable
y_var <- df[,"SiriBF."]
cv_lasso <- cv.glmnet(x_var, y_var, alpha = 1)
cv_lasso
plot(cv_lasso)
x_var <- data.matrix(df[,-1])     # NOTE: glmnet requires a matrix structure
# getting the response variable
y_var <- df[,"SiriBF."]
cv_lasso <- cv.glmnet(x_var, y_var, alpha = 1)
cv_lasso
plot(cv_lasso)
x_var <- data.matrix(df[,-1])     # NOTE: glmnet requires a matrix structure
# getting the response variable
y_var <- df[,"SiriBF."]
cv_lasso <- cv.glmnet(x_var, y_var, alpha = 1)
cv_lasso
plot(cv_lasso)
min_lasso <- glmnet(x_var, y_var, alpha=1, lambda= cv_lasso$lambda.min)
coef(min_lasso)
lasso_coefs
x_var <- data.matrix(df[,-1])     # NOTE: glmnet requires a matrix structure
# getting the response variable
y_var <- df[,"SiriBF."]
cv_lasso <- cv.glmnet(x_var, y_var, alpha = 1)
cv_lasso
plot(cv_lasso)
min_lasso <- glmnet(x_var, y_var, alpha=1, lambda= cv_lasso$lambda.min)
lasso_coefs <- coef(min_lasso)
lasso_coefs
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) # just to clean the pdf files, not recommended in practice!
library(caret)  # statistical learning techniques
library(leaps)  # BSS
library(glmnet)
df <- read.table('BODY_FAT.TXT', header=TRUE)
names(df)
df <- df[,-1]
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) # just to clean the pdf files, not recommended in practice!
library(caret)  # statistical learning techniques
library(leaps)  # BSS
library(glmnet)
df <- read.table('BODY_FAT.TXT', header=TRUE)
names(df)
df <- df[,-1]
wrap: s
library(caret)  # statistical learning techniques
library(leaps)  # BSS
library(glmnet)
---
title: "*Feature Selection*"
---
title: "*Feature Selection*"
df <- df[,-1]
df <- read.table('BODY_FAT.TXT', header=TRUE)
names(df)
x_var <- data.matrix(df[,-1])     # NOTE: glmnet requires a matrix structure
# getting the response variable
y_var <- df[,"SiriBF."]
cv_lasso <- cv.glmnet(x_var, y_var, alpha = 1)
cv_lasso
plot(cv_lasso)
min_lasso <- glmnet(x_var, y_var, alpha=1, lambda= cv_lasso$lambda.min)
lasso_coefs <- coef(min_lasso)
lasso_coefs
df <- read.table('BODY_FAT.TXT', header=TRUE)
names(df)
df <- df[,-1]
x_var <- data.matrix(df[,-1])     # NOTE: glmnet requires a matrix structure
# getting the response variable
y_var <- df[,"SiriBF."]
cv_lasso <- cv.glmnet(x_var, y_var, alpha = 1)
cv_lasso
plot(cv_lasso)
min_lasso <- glmnet(x_var, y_var, alpha=1, lambda= cv_lasso$lambda.min)
lasso_coefs <- coef(min_lasso)
lasso_coefs
regfit.full = regsubsets(SiriBF. ~ ., data = df,  nvmax = 13, method="exhaustive")
summary(regfit.full)
names(summary(regfit.full))
summary(regfit.full)$rsq
#plot rss
plot(summary(regfit.full)$rsq, type="b")
reg.summary <- summary(regfit.full)
par(mfrow=c(2,2))
plot(reg.summary$rss ,xlab="Number of Variables ",ylab="RSS",type="l")
plot(reg.summary$adjr2 ,xlab="Number of Variables ", ylab="Adjusted RSq",type="l")
max_adjr2 <- which.max(reg.summary$adjr2)
abline(v=max_adjr2, col="red", lty=2)
points(max_adjr2,reg.summary$adjr2[max_adjr2], col="red",cex=2,pch=20)
plot(reg.summary$cp ,xlab="Number of Variables ",ylab="Cp", type='l')
min_cp <- which.min(reg.summary$cp )
points(min_cp, reg.summary$cp[min_cp],col="red",cex=2,pch=20)
abline(v=min_cp, col="red", lty=2)
plot(reg.summary$bic ,xlab="Number of Variables ",ylab="BIC",type='l')
min_bic <- which.min(reg.summary$bic)
points(min_bic,reg.summary$bic[min_bic],col="red",cex=2,pch=20)
abline(v=min_bic, col="red", lty=2)
plot(regfit.full,scale="bic") #for "bic"
plot(regfit.full,scale="adjr2") #for "adjr2"
dim(df)
set.seed(1)
train=sample(1:nrow(df), round(nrow(df)*0.8), rep=F)
test = which(!(1:nrow(df) %in% train))
length(test)
length(train)
regfit.best <- regsubsets(SiriBF. ~ ., data = df[train,],  nvmax = 13, method="exhaustive")
test.mat = model.matrix(SiriBF. ~ ., data = df[test,])
dim(test.mat)
p = (ncol(df)-1) # number of predictors
mse <- rep(NA, p) # out-of-sample MSE
for(i in 1:p){
coefi=coef(regfit.best,id=i)
pred=test.mat[,names(coefi)]%*%coefi
mse[i]=mean((df$SiriBF.[test]-pred)^2)
}
plot(mse, type='b')
abline(v=which.min(mse), col="red", lty=2)
legend(x = "bottomright",
legend = "min test-MSPE",
lty = 2,
col = "red")
coef(regfit.best, which.min(mse))
regfit.best.full <- regsubsets(SiriBF. ~ ., data=df ,nvmax=13)
coef(regfit.best.full, which.min(mse))
rescompare <- rbind(coef(regfit.best, which.min(mse)),
coef(regfit.best.full, which.min(mse)))
rownames(rescompare) <- c("train", "full")
rescompare
k = 10
set.seed(123)
# folds
folds <- createFolds(1:nrow(df), k = 10, list = TRUE, returnTrain = T)
folds
k = 10
set.seed(123)
# folds
folds <- createFolds(1:nrow(df), k = 10, list = TRUE, returnTrain = T)
fold <- matrix(NA, nrow(df), k)
for (i in 1:k) {
fold[, i] <- (1:nrow(df) %in% folds[[i]])
}
head(fold, 10)
# initialize an empty matrix to contain test errors
cv.errors=matrix(NA, k, # num of folds
p,     # num of variables
dimnames=list(NULL, paste(1:p)))
predict.regsubsets = function (object, newdata, id, ...){
form = as.formula(object$call[[2]])
mat = model.matrix(form,newdata)
coefi = coef(object ,id=id)
xvars = names(coefi)
mat[,xvars] %*% coefi
}
# loop for each fold
for(j in 1:k){
best.fit = regsubsets(SiriBF. ~ . , data=df[fold[,j], ], nvmax = p)
# for each best model
for (i in 1:p){
pred = predict.regsubsets(best.fit, df[!fold[,j], ], id = i)
cv.errors[j, i] = mean((df$SiriBF.[!fold[,j]] - pred)^2)
}
}
mean_mse <- colMeans(cv.errors)
plot(mean_mse, type='b')
abline(v=which.min(mean_mse), col="red", lty=2)
legend(x = "bottomright",
legend = "min CV-MSPE",
lty = 2,
col = "red")
reg.best  <- regsubsets (SiriBF. ~ ., data=df, nvmax=p)
coef(reg.best, which.min(mean_mse))
regfit.fwd = regsubsets(SiriBF. ~. , data=df,nvmax=13, method ="forward")
regfit.bwd = regsubsets(SiriBF. ~. , data=df,nvmax=13, method ="backward")
summary(regfit.fwd)
summary(regfit.bwd)
